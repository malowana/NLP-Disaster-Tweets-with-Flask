{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-disasterTweetsv2.ipynb",
      "provenance": [],
      "mount_file_id": "1rPN9tcJfJX18rRPAupwyDiz45KAZNbVK",
      "authorship_tag": "ABX9TyNXBigCBpL4ojTzQAIKjJ3f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malowana/NLP-Disaster-Tweets-with-Flask/blob/main/NLP_disasterTweetsv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEXg0Gq1w5Xw"
      },
      "source": [
        "!pip install scikit-plot\n",
        "!pip install neptune-client\n",
        "!pip install opendatasets --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGegARvmGLGM"
      },
      "source": [
        "!pip install texthero"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smjKOfzDIVqA"
      },
      "source": [
        "!pip3 install catboost\n",
        "!pip install scikit-plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuSrPoA6xE-q"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import neptune\n",
        "import neptune.new as neptune\n",
        "import opendatasets as od\n",
        "import texthero as hero\n",
        "import pickle\n",
        "\n",
        "from gensim.utils import simple_preprocess\n",
        "import catboost as ctb\n",
        "import xgboost as xgb\n",
        "from gensim.models import FastText\n",
        "from scikitplot.estimators import plot_learning_curve\n",
        "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\n",
        "from gensim.models import Word2Vec, Phrases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj6mPqwxB5xO"
      },
      "source": [
        "## **Load data and initial check**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmgm69dAxGqG"
      },
      "source": [
        "# Kaggle authorization data: {'username': '...', 'key': '...'}\n",
        "with open('/content/drive/MyDrive/Konkursy Kaggle/Disaster Tweets/kaggle.json') as f:\n",
        "   kaggle_authorization = json.load(f)\n",
        "\n",
        "# Neptune authorization data: {'project': '...', 'api_token': '...'}\n",
        "with open('/content/drive/MyDrive/Konkursy Kaggle/Disaster Tweets/neptune-disastertweets.json') as f:\n",
        "   neptune_authorization = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3z3dRNTJ50lk",
        "outputId": "53bc39c5-7370-48a6-ea0c-fcfc15d6ddde"
      },
      "source": [
        "# Token should be set in the system settings or directly below\n",
        "#PROJECT_NAME = 'DataWorkshop-Foundation/Predicting-cancellation-of-visit' \n",
        "PROJECT_NAME = neptune_authorization['project'] \n",
        "MY_TOKEN = neptune_authorization['api_token']\n",
        "project = neptune.init(project = PROJECT_NAME, api_token=MY_TOKEN)\n",
        "project"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://app.neptune.ai/paulina21/Kaggle-Disaster-Tweets/e/KAG-25\n",
            "Remember to stop your run once you’ve finished logging your metadata (https://docs.neptune.ai/api-reference/run#stop). It will be stopped automatically only when the notebook kernel/interactive console is terminated.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<neptune.new.run.Run at 0x7f09dc4e4b90>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aAge8kQyx4M"
      },
      "source": [
        "kaggle_url = \"https://www.kaggle.com/c/nlp-getting-started/data\"\n",
        "filename_test = r\"/content/username/nlp-getting-started/test.csv\"\n",
        "filename_train = r\"/content/username/nlp-getting-started/train.csv\"\n",
        "path = r\".\"  # for saving data\n",
        "author = \"Paulina Sz\"  # for Neptune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35tGYjwl1FqT",
        "outputId": "538afda7-2c80-4ed6-e442-8b85ba15b594"
      },
      "source": [
        "# Upload directly from Kaggle:\n",
        "od.download(kaggle_url, *kaggle_authorization)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: paulinasz\n",
            "Your Kaggle Key: ··········\n",
            "Downloading nlp-getting-started.zip to username/nlp-getting-started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 593k/593k [00:00<00:00, 51.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Extracting archive username/nlp-getting-started/nlp-getting-started.zip to username/nlp-getting-started\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "ExDyaQcF7f82",
        "outputId": "88657b2f-89a1-48e9-8794-642c6a3319e4"
      },
      "source": [
        "df_train = pd.read_csv(filename_train)\n",
        "df_test = pd.read_csv(filename_test)\n",
        "print(\"Shape:\", df_train.shape)\n",
        "print(\"Duplicated:\", df_train.duplicated().sum())\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape: (7613, 5)\n",
            "Duplicated: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdhKlszYCMzf",
        "outputId": "3886f051-245d-4de8-be7b-51126973d239"
      },
      "source": [
        "df_train.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 7613 entries, 0 to 7612\n",
            "Data columns (total 5 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   id        7613 non-null   int64 \n",
            " 1   keyword   7552 non-null   object\n",
            " 2   location  5080 non-null   object\n",
            " 3   text      7613 non-null   object\n",
            " 4   target    7613 non-null   int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 297.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QR30zI2JBNBe",
        "outputId": "d128dd54-9ff4-4e20-a0e6-ab305d98ed83"
      },
      "source": [
        "# Check target (ratio)\n",
        "df_train[\"target\"].value_counts(normalize=True, dropna=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    0.57034\n",
              "1    0.42966\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WD_1rjlNBrBp",
        "outputId": "50bcda85-5985-420f-af35-848f051dd048"
      },
      "source": [
        "#check if missing values\n",
        "def is_missing(train):\n",
        "  for column in train.columns:\n",
        "      missing = column, train[column].isnull().sum()\n",
        "      if missing[1] == 0: continue\n",
        "      print(missing)\n",
        "\n",
        "is_missing(df_train)\n",
        "is_missing(df_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('keyword', 61)\n",
            "('location', 2533)\n",
            "('keyword', 26)\n",
            "('location', 1105)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJow7_ORE6-T"
      },
      "source": [
        "#concatenate the subsets\n",
        "all = pd.concat([df_train, df_test], sort=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kVwy9NmCldo"
      },
      "source": [
        "## **Feature engineering**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBbr8fWAM0O5"
      },
      "source": [
        "def get_all_hastags(list):\n",
        "    for word in list:\n",
        "      if word.startswith('#'):\n",
        "        return word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "sHrKyvdPI1St",
        "outputId": "59a84c3f-0d63-48b4-c974-192cb0722944"
      },
      "source": [
        "#fill missing keywords from # from text\n",
        "all[\"text_list\"] = all[\"text\"].map(lambda x: x.split())\n",
        "all[\"text_hashtags\"] = all[\"text_list\"].map(get_all_hastags)\n",
        "all['keyword'].fillna(all[\"text_hashtags\"], inplace=True)\n",
        "if 'text_hashtags' in all: del all['text_hashtags']\n",
        "all.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>text_list</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>#earthquake</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, #eart...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask., Canada]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[All, residents, asked, to, 'shelter, in, plac...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>#wildfires</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>#Alaska</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, #Al...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id      keyword  ... target                                          text_list\n",
              "0   1  #earthquake  ...    1.0  [Our, Deeds, are, the, Reason, of, this, #eart...\n",
              "1   4         None  ...    1.0     [Forest, fire, near, La, Ronge, Sask., Canada]\n",
              "2   5         None  ...    1.0  [All, residents, asked, to, 'shelter, in, plac...\n",
              "3   6   #wildfires  ...    1.0  [13,000, people, receive, #wildfires, evacuati...\n",
              "4   7      #Alaska  ...    1.0  [Just, got, sent, this, photo, from, Ruby, #Al...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rgzb5S7EPvC4"
      },
      "source": [
        "all.location = all.location.fillna(\"other\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMprd-c8Chj7"
      },
      "source": [
        "#location\n",
        "all.loc[all['location'].str.contains('Iraq|Pakistan'), 'location'] = 'Asia'\n",
        "all.loc[all['location'].str.contains('Las Vegas'), 'location'] = 'Las Vegas'\n",
        "all.loc[all['location'].str.contains('Irleand|Dublin'), 'location'] = 'Irleand'\n",
        "all.loc[all['location'].str.contains('OH|Ohio'), 'location'] = 'Ohio'\n",
        "all.loc[all['location'].str.contains('Brazil'), 'location'] = 'Brasil'\n",
        "all.loc[all['location'].str.contains('MO|Kansas City'), 'location'] = 'Missouri'\n",
        "all.loc[all['location'].str.contains('umbai|India'), 'location'] = 'India'\n",
        "all.loc[all['location'].str.contains('Italy|France|Switzerland|Netherlands|Germany|Sweden|Geneva|Rome|Spain'), 'location'] = 'Europe'\n",
        "all.loc[all['location'].str.contains('Nigeria|Lagos'), 'location'] = 'Nigeria'\n",
        "all.loc[all['location'].str.contains('Japan|Tokyo'), 'location'] = 'Japan'\n",
        "all.loc[all['location'].str.contains('Virginia|VA'), 'location'] = 'Virginia'\n",
        "all.loc[all['location'].str.contains('Hong Kong|china|China'), 'location'] = 'China'\n",
        "all.loc[all['location'].str.contains('Edinburgh'), 'location'] = 'Scotland'\n",
        "all.loc[all['location'].str.contains('NC|North Carolina'), 'location'] = 'North Carolina'\n",
        "all.loc[all['location'].str.contains('Portland'), 'location'] = 'Portland'\n",
        "all.loc[all['location'].str.contains('Boston|WI|PA|IA|Poenix|Georgia|America|United States|Unites States|U.S.A|US|U.S.|Wisconsin|New Hampshire|Arizona|Midwest|Indiana|Maryland|Pennsylvania|Puerto Rico|Oregon|Michigan'), 'location'] = 'USA'\n",
        "all.loc[all['location'].str.contains('NJ|NY|New York|NYC|york|nyc'), 'location'] = 'New York'\n",
        "all.loc[all['location'].str.contains('Sydney'), 'location'] = 'Sydney'\n",
        "all.loc[all['location'].str.contains('ondon|United Kingdom|England|Coventry|Birmingham|Paignton|Liverpool|Glasgow|Newcastle|UK|Manchester'), 'location'] = 'UK'\n",
        "all.loc[all['location'].str.contains('Australia|Melbourne|Sydney'), 'location'] = 'Australia'\n",
        "all.loc[all['location'].str.contains('California|San Diego|Sacramento|San Francisco'), 'location'] = 'California'\n",
        "all.loc[all['location'].str.contains('Washington'), 'location'] = 'Washington'\n",
        "all.loc[all['location'].str.contains('Chicago'), 'location'] = 'Chicago'\n",
        "all.loc[all['location'].str.contains('Los Angeles|Subconscious|LA'), 'location'] = 'Los Angeles'\n",
        "all.loc[all['location'].str.contains('Seattle, WA'), 'location'] = 'Seattle'\n",
        "all.loc[all['location'].str.contains('World|orldwide|Earth|Global|The Universe'), 'location'] = 'Everywhere'\n",
        "all.loc[all['location'].str.contains('TX|Texas|Houston'), 'location'] = 'Texas'\n",
        "all.loc[all['location'].str.contains('CA|Canada|Toronto|Vancouver|canada|Calgary'), 'location'] = 'Canada'\n",
        "all.loc[all['location'].str.contains('TN'), 'location'] = 'Tennessee'\n",
        "all.loc[all['location'].str.contains('FL|Orlando'), 'location'] = 'Florida'\n",
        "all.loc[all['location'].str.contains('304|house|hell|Taylor|Tunes|stairs|Coffee|ss|Money|Happily Married|MAD as Hell|Billionaires|Word|Pedophile|Narnia|Your screen|Here|Jupiter|Anonymous|really|Breaking|where|All|Some|Innerhalb|Sky|BROKE|Can|Sky'), 'location'] = 'other'\n",
        "all.loc[all['location'].str.contains('Lagos'), 'location'] = 'Lagos'\n",
        "all.loc[all['location'].str.contains('Denver'), 'location'] = 'Denver'\n",
        "all.loc[all['location'].str.contains('New Jersey'), 'location'] = 'New Jersey'\n",
        "all.loc[all['location'].str.contains('Calgary'), 'location'] = 'Calgary'\n",
        "all.loc[all['location'].str.contains('Atlanta|atlanta'), 'location'] = 'Atlanta'\n",
        "all.loc[all['location'].str.contains('Detroit'), 'location'] = 'Michigan'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENZcWi32QmPn"
      },
      "source": [
        "def new_features(all):\n",
        "  all['length_text'] = all['text'].str.len()\n",
        "  all['length_keyword'] = all['keyword'].str.len()\n",
        "  all['length_location'] = all['location'].str.len()\n",
        "  all['word_count'] = all['text'].str.split().str.len()\n",
        "  all['is_upper_text'] = all['text'].str.count(r'[A-Z]')\n",
        "  all['is_digit_text'] = all['text'].str.count(r'[0-9]')\n",
        "  all['%_upper_text'] = all['is_upper_text']/all['length_text']\n",
        "  all['is_#'] = all['text'].str.count(r'#')\n",
        "  all.loc[(all['text'].str.contains(\"http\")), 'is_url'] = 1\n",
        "  all.loc[all['is_url'].isnull(), 'is_url'] = 0\n",
        "  all['is_@'] = all['text'].str.count(r'@')\n",
        "  all['is_?'] = all['text'].str.count(r'\\?')\n",
        "  all['is_!'] = all['text'].str.count(r'\\!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC68Hsn9RntQ"
      },
      "source": [
        "new_features(all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lWAnGsNRsqq"
      },
      "source": [
        "import re\n",
        "def re_urls(text, replace_for=\"URL\"):\n",
        "    return re.sub(r'https?://\\S+', replace_for, text) \n",
        "\n",
        "def re_digits(text, replace_for=' '): \n",
        "    result = re.sub(r'\\d+', replace_for, text) \n",
        "    return result\n",
        "\n",
        "def re_hashtags(text, replace_for=' '): \n",
        "    result = re.sub(r'#', replace_for, text) \n",
        "    return result\n",
        "\n",
        "def preprocessing(doc):\n",
        "    doc = re_urls(doc)\n",
        "    doc = re_digits(doc)\n",
        "    doc = re_hashtags(doc)\n",
        "    return doc\n",
        "\n",
        "all['text_after_preprocessing'] = all['text'].map(preprocessing).values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "hE0ErYtcSGQ4",
        "outputId": "75ae29a5-968b-4a7f-de33-981f1cabb621"
      },
      "source": [
        "all[all.location==\"other\"].sample(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>text_list</th>\n",
              "      <th>length_text</th>\n",
              "      <th>length_keyword</th>\n",
              "      <th>length_location</th>\n",
              "      <th>word_count</th>\n",
              "      <th>is_upper_text</th>\n",
              "      <th>is_digit_text</th>\n",
              "      <th>%_upper_text</th>\n",
              "      <th>is_#</th>\n",
              "      <th>is_url</th>\n",
              "      <th>is_@</th>\n",
              "      <th>is_?</th>\n",
              "      <th>is_!</th>\n",
              "      <th>text_after_preprocessing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6951</th>\n",
              "      <td>9973</td>\n",
              "      <td>tsunami</td>\n",
              "      <td>other</td>\n",
              "      <td>#sing #tsunami Beginners #computer tutorial.: ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[#sing, #tsunami, Beginners, #computer, tutori...</td>\n",
              "      <td>133</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>20</td>\n",
              "      <td>1</td>\n",
              "      <td>0.150376</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>sing  tsunami Beginners  computer tutorial.: ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5103</th>\n",
              "      <td>7280</td>\n",
              "      <td>nuclear%20disaster</td>\n",
              "      <td>other</td>\n",
              "      <td>Any disaster impairs mental health especially ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[Any, disaster, impairs, mental, health, espec...</td>\n",
              "      <td>97</td>\n",
              "      <td>18.0</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.051546</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Any disaster impairs mental health especially ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5421</th>\n",
              "      <td>7737</td>\n",
              "      <td>panicking</td>\n",
              "      <td>other</td>\n",
              "      <td>When he lets you drive his truck and you start...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[When, he, lets, you, drive, his, truck, and, ...</td>\n",
              "      <td>124</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5</td>\n",
              "      <td>20</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>0.040323</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>When he lets you drive his truck and you start...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5958</th>\n",
              "      <td>8509</td>\n",
              "      <td>screaming</td>\n",
              "      <td>other</td>\n",
              "      <td>/ it's fine baby I was screaming at the TV x h...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>[/, it's, fine, baby, I, was, screaming, at, t...</td>\n",
              "      <td>68</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5</td>\n",
              "      <td>12</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0.147059</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>/ it's fine baby I was screaming at the TV x URL</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3348</th>\n",
              "      <td>4793</td>\n",
              "      <td>evacuated</td>\n",
              "      <td>other</td>\n",
              "      <td>Hotel evacuated after fire on Shore Drive in N...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[Hotel, evacuated, after, fire, on, Shore, Dri...</td>\n",
              "      <td>75</td>\n",
              "      <td>9.0</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Hotel evacuated after fire on Shore Drive in N...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ...                           text_after_preprocessing\n",
              "6951  9973  ...   sing  tsunami Beginners  computer tutorial.: ...\n",
              "5103  7280  ...  Any disaster impairs mental health especially ...\n",
              "5421  7737  ...  When he lets you drive his truck and you start...\n",
              "5958  8509  ...   / it's fine baby I was screaming at the TV x URL\n",
              "3348  4793  ...  Hotel evacuated after fire on Shore Drive in N...\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "2WzD7iCcGA2n",
        "outputId": "898be418-92e3-4d10-eaf1-0d5f4ca0b0f1"
      },
      "source": [
        "all[\"text_hero\"] = hero.clean(all[\"text_after_preprocessing\"])\n",
        "all.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>text_list</th>\n",
              "      <th>length_text</th>\n",
              "      <th>length_keyword</th>\n",
              "      <th>length_location</th>\n",
              "      <th>word_count</th>\n",
              "      <th>is_upper_text</th>\n",
              "      <th>is_digit_text</th>\n",
              "      <th>%_upper_text</th>\n",
              "      <th>is_#</th>\n",
              "      <th>is_url</th>\n",
              "      <th>is_@</th>\n",
              "      <th>is_?</th>\n",
              "      <th>is_!</th>\n",
              "      <th>text_after_preprocessing</th>\n",
              "      <th>text_hero</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>#earthquake</td>\n",
              "      <td>other</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[Our, Deeds, are, the, Reason, of, this, #eart...</td>\n",
              "      <td>69</td>\n",
              "      <td>11.0</td>\n",
              "      <td>5</td>\n",
              "      <td>13</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.144928</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Our Deeds are the Reason of this  earthquake M...</td>\n",
              "      <td>deeds reason earthquake may allah forgive us</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>None</td>\n",
              "      <td>other</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[Forest, fire, near, La, Ronge, Sask., Canada]</td>\n",
              "      <td>38</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0.131579</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>forest fire near la ronge sask canada</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>None</td>\n",
              "      <td>other</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[All, residents, asked, to, 'shelter, in, plac...</td>\n",
              "      <td>133</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5</td>\n",
              "      <td>22</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0.015038</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>residents asked shelter place notified officer...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>#wildfires</td>\n",
              "      <td>other</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
              "      <td>65</td>\n",
              "      <td>10.0</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0.015385</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>,  people receive  wildfires evacuation order...</td>\n",
              "      <td>people receive wildfires evacuation orders cal...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>#Alaska</td>\n",
              "      <td>other</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>[Just, got, sent, this, photo, from, Ruby, #Al...</td>\n",
              "      <td>88</td>\n",
              "      <td>7.0</td>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.034091</td>\n",
              "      <td>2</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Just got sent this photo from Ruby  Alaska as ...</td>\n",
              "      <td>got sent photo ruby alaska smoke wildfires pou...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  ...                                          text_hero\n",
              "0   1  ...       deeds reason earthquake may allah forgive us\n",
              "1   4  ...              forest fire near la ronge sask canada\n",
              "2   5  ...  residents asked shelter place notified officer...\n",
              "3   6  ...  people receive wildfires evacuation orders cal...\n",
              "4   7  ...  got sent photo ruby alaska smoke wildfires pou...\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSPSa0CGGfOK"
      },
      "source": [
        "#all_1 = all[all['target']==0]\n",
        "#all_0 = all[all['target']==1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmbSDpclG-vr"
      },
      "source": [
        "#hero.visualization.wordcloud(all_0[\"text_hero\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNsj1wTGHIJ2"
      },
      "source": [
        "#hero.visualization.wordcloud(all_1[\"text_hero\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GbVwVDRVa91"
      },
      "source": [
        "## **Create models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMBpi8zkKjrh"
      },
      "source": [
        "def get_feats(df, black_list=['target', 'id' ]):\n",
        "  num_feats = df.select_dtypes(np.number).columns\n",
        "  return [x for x in num_feats if x not in black_list]\n",
        "\n",
        "#vectorization & vectors average\n",
        "def get_doc2vec_X(model, tokens):\n",
        "    def __calc_doc2vec(words):   #vectorization\n",
        "        return np.mean([model.wv[w] for w in words if w in model.wv], axis=0)\n",
        "\n",
        "    X = tokens.map(__calc_doc2vec)\n",
        "    default_vector = X[ False == X.isnull() ].mean()\n",
        "    return np.stack( X.map(lambda x: default_vector if str(x) == 'nan' else x) )\n",
        "\n",
        "def draw_feature_importances(model, features):\n",
        "    importances = model.feature_importances_\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.title(\"Feature importances\")\n",
        "    plt.bar(range(X.shape[1]), model.feature_importances_[indices],\n",
        "           color=\"b\", align=\"center\")\n",
        "    plt.xticks(range(X.shape[1]), [ features[x] for x in indices])\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.xlim([-1, X.shape[1]])\n",
        "    plt.show()\n",
        "\n",
        "def log_to_neptune(model, params, feats, tags,metric):\n",
        "  project = neptune.init(project = PROJECT_NAME, api_token=MY_TOKEN)\n",
        "  project[\"model\"] = params\n",
        "  project[\"parameters\"] = params\n",
        "  project[\"feats\"] = feats\n",
        "  project[\"sys/tags\"].add(tags)\n",
        "  project[\"train/accuracy\"] = metric\n",
        "  project.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Phjy5ymPVeW_"
      },
      "source": [
        "#split to test/train again\n",
        "train2 = all[ ~all['target'].isnull() ].copy()\n",
        "test2 = all[ all['target'].isnull() ].copy()\n",
        "\n",
        "#tokenization\n",
        "train2['simple_tokens'] = train2['text_hero'].map(simple_preprocess)\n",
        "#output\n",
        "y = train2['target']\n",
        "#num feats\n",
        "feats = get_feats(train2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkqpIn9hwAkI"
      },
      "source": [
        "# Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj9UKk2B8pnj"
      },
      "source": [
        "\n",
        "model_w2v = Word2Vec(train2['simple_tokens'], size=50, window=3, seed=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rwy8ZNuWZY7",
        "outputId": "61208caf-e0c7-4059-f610-732ccbc49411"
      },
      "source": [
        "print(model_w2v)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec(vocab=2680, size=50, alpha=0.025)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjXjsI5EWdpm",
        "outputId": "107a4da0-96b4-4bf1-c1d7-877e27fb90bc"
      },
      "source": [
        "X_w2v_embedding = get_doc2vec_X(model_w2v, train2['simple_tokens'])\n",
        "print(X_w2v_embedding)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning:\n",
            "\n",
            "Mean of empty slice.\n",
            "\n",
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_methods.py:170: RuntimeWarning:\n",
            "\n",
            "invalid value encountered in double_scalars\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.10299411 -0.40191525  0.20049639 ...  0.32855585  0.19167137\n",
            "   0.5453486 ]\n",
            " [ 0.10710851 -0.4194923   0.20801027 ...  0.34525248  0.20134237\n",
            "   0.5757696 ]\n",
            " [ 0.04356027 -0.16717868  0.08565561 ...  0.14018156  0.08618589\n",
            "   0.23345327]\n",
            " ...\n",
            " [ 0.09478268 -0.3711911   0.18791606 ...  0.30046025  0.17895092\n",
            "   0.5035943 ]\n",
            " [ 0.07152266 -0.2928303   0.14313339 ...  0.23178186  0.13906536\n",
            "   0.40198767]\n",
            " [ 0.12232207 -0.46423885  0.23901184 ...  0.38720113  0.22838014\n",
            "   0.64138347]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmfL0d2nWs62"
      },
      "source": [
        "X_w2v_embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgxQi3bAYLvv"
      },
      "source": [
        "#word2vec\n",
        "model_w2v = Word2Vec(train2['simple_tokens'], size=50, window=3, seed=0)\n",
        "X_w2v_embedding = get_doc2vec_X(model_w2v, train2['simple_tokens'])\n",
        "train2_w2v_encoding = pd.DataFrame(X_w2v_embedding)\n",
        "X_w2v_embedding_feats = pd.merge(train2_w2v_encoding,train2[feats],left_index=True, right_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKNp8X5pwDdK"
      },
      "source": [
        "#fasttext\n",
        "model = FastText(train2['simple_tokens'], size=100, window=4, seed=0)\n",
        "\n",
        "X_fasttext_embedding = get_doc2vec_X(model, train2['simple_tokens'])\n",
        "train2_fasttext_encoding = pd.DataFrame(X_fasttext_embedding)\n",
        "X_fasttext_embedding_feats = pd.merge(train2_fasttext_encoding,train2[feats],left_index=True, right_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiKzQdApH39X"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iWb1XVNibUf"
      },
      "source": [
        "catboost = ctb.CatBoostClassifier(max_depth=8, n_estimators=100, verbose=0, random_state=0, custom_metric='AUC')\n",
        "xgboost = xgb.XGBClassifier(max_depth=8, n_estimators=100, verbose=0, random_state=0, custom_metric='AUC')\n",
        "\n",
        "def train_model(main_model, X_value, embedding_model, feats, tags):\n",
        "    w2v_model_xgb = main_model\n",
        "    w2v_model_xgb.fit(X_value, y)\n",
        "    plot_learning_curve(w2v_model_xgb, X_value, y, cv=3, random_state=2019, shuffle=True)\n",
        "\n",
        "    cvs = cross_val_score(w2v_model_xgb, X_value, y, scoring='accuracy', cv=3).mean()\n",
        "    print(cvs)\n",
        "\n",
        "    log_to_neptune(embedding_model, main_model, feats, tags, cvs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IhkBkSfbcIg0"
      },
      "source": [
        "word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M89CDyerb0K9"
      },
      "source": [
        "#only text_hero, ctb\n",
        "w2v_model_ctb = ctb.CatBoostClassifier(max_depth=8, n_estimators=100, verbose=0, random_state=0, custom_metric='AUC')\n",
        "w2v_model_ctb.fit(X_w2v_embedding, y)\n",
        "plot_learning_curve(w2v_model_ctb, X_w2v_embedding, y, cv=3, random_state=2019, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlzkqiAeBbyH"
      },
      "source": [
        "w2v_model_ctb.save_model('/content/drive/MyDrive/Konkursy Kaggle/Disaster Tweets/w2v_model_ctb_new')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdfkFO5pcrFf"
      },
      "source": [
        "cvs = cross_val_score(w2v_model_ctb, X_w2v_embedding, y, scoring='accuracy', cv=3).mean()\n",
        "print(cvs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bKOkwyvcoRc"
      },
      "source": [
        "log_to_neptune(\"word2vec\", w2v_model_ctb, ['text_hero'], ['w2v', 'text_hero', 'ctb'], cvs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfqeECNGdYRp"
      },
      "source": [
        "#text_hero+ feature engineering, ctb\n",
        "w2v_model_ctb = ctb.CatBoostClassifier(max_depth=8, n_estimators=100, verbose=0, random_state=0, custom_metric='AUC')\n",
        "w2v_model_ctb.fit(X_w2v_embedding_feats, y)\n",
        "plot_learning_curve(w2v_model_ctb, X_w2v_embedding_feats, y, cv=3, random_state=2019, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yej5axQWdmBp"
      },
      "source": [
        "cvs = cross_val_score(w2v_model_ctb, X_w2v_embedding_feats, y, scoring='accuracy', cv=3).mean()\n",
        "print(cvs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LsQFs5odpYV"
      },
      "source": [
        "log_to_neptune(\"word2vec\", \"ctb.CatBoostClassifier(max_depth=8, n_estimators=100, verbose=0, random_state=0, custom_metric='AUC')\", ['text_hero'], ['w2v', 'feature_engineering', 'ctb'], cvs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ0xeHBRfp5F"
      },
      "source": [
        "#only text_hero, xgb\n",
        "w2v_model_xgb = xgb.XGBClassifier(max_depth=8, n_estimators=100, verbose=0, random_state=0, custom_metric='AUC')\n",
        "w2v_model_xgb.fit(X_w2v_embedding, y)\n",
        "plot_learning_curve(w2v_model_xgb, X_w2v_embedding, y, cv=3, random_state=2019, shuffle=True)\n",
        "\n",
        "cvs = cross_val_score(w2v_model_xgb, X_w2v_embedding, y, scoring='accuracy', cv=3).mean()\n",
        "print(cvs)\n",
        "\n",
        "log_to_neptune(\"word2vec\", \"xgb.XGBClassifier(max_depth=8, n_estimators=100, verbose=0, random_state=0, custom_metric='AUC')\", ['text_hero'], ['w2v', 'text_hero', 'xgb'], cvs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnrrKAxWge48"
      },
      "source": [
        "#text_hero+ feats, xgb\n",
        "w2v_model_xgb = xgb.XGBClassifier(max_depth=8, n_estimators=100, verbose=0, random_state=0, custom_metric='AUC')\n",
        "w2v_model_xgb.fit(X_w2v_embedding_feats, y)\n",
        "plot_learning_curve(w2v_model_xgb, X_w2v_embedding_feats, y, cv=3, random_state=2019, shuffle=True)\n",
        "\n",
        "cvs = cross_val_score(w2v_model_xgb, X_w2v_embedding_feats, y, scoring='accuracy', cv=3).mean()\n",
        "print(cvs)\n",
        "\n",
        "log_to_neptune(\"word2vec\", \"xgb.XGBClassifier(max_depth=8, n_estimators=100, verbose=0, random_state=0, custom_metric='AUC')\", feats, ['w2v', 'feature_engineering', 'xgb'], cvs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNmGacKsjJSs"
      },
      "source": [
        "train_model(xgboost, X_w2v_embedding_feats, \"word2vec\", feats, ['w2v', 'feature_engineering', 'xgb'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezzqgKQxcmZg"
      },
      "source": [
        "Fasttext"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8f2zw9HpH2q-"
      },
      "source": [
        "#only text_hero, ctb\n",
        "fasttext_model = ctb.CatBoostClassifier(max_depth=8, n_estimators=100, verbose=0, random_state=0, custom_metric='AUC')\n",
        "fasttext_model.fit(X, y)\n",
        "plot_learning_curve(fasttext_model, X, y, cv=3, random_state=2019, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pIKykvlWYs6"
      },
      "source": [
        "\"\"\"\n",
        "project[\"parameters\"] = \"max_depth:8, n_estimators:100, verbose:0, random_state:0, custom_metric:'AUC'\"\n",
        "project[\"feats\"] = [\"text_hero\"]\n",
        "project[\"sys/tags\"].add([\"fasttext\", \"text_hero\"])\n",
        "project[\"train/accuracy\"] = 0.62\n",
        "project.stop()\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJ8EvJrNORpo"
      },
      "source": [
        "cross_val_score(fasttext_model, X, y, scoring='accuracy', cv=3).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vT63mut3LSa2"
      },
      "source": [
        "#text_hero+numerical, ctb\n",
        "fasttext_model2 = ctb.CatBoostClassifier(max_depth=8, n_estimators=100, verbose=0, random_state=0, custom_metric='AUC')\n",
        "fasttext_model2.fit(X_all, y)\n",
        "#plot_learning_curve(fasttext_model2, X_all, y, cv=3, random_state=2019, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuNN0dXTON1G"
      },
      "source": [
        "cvs = cross_val_score(fasttext_model2, X_all, y, scoring='accuracy', cv=3).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3mtFM72dXZj"
      },
      "source": [
        "#params = \"max_depth=8, n_estimators=100, verbose=0, random_state=0, custom_metric='AUC'\"\n",
        "#log_to_neptune(fasttext_model2, params, feats, ['fasttext', 'text_hero', 'feature_engineering', 'ctb'], cvs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy3KMSSHLfZs"
      },
      "source": [
        "#only text_hero, xgb\n",
        "fasttext_model_xgb = xgb.XGBClassifier(max_depth=8, n_estimators=100, verbose=0, random_state=0, custom_metric='AUC')\n",
        "fasttext_model_xgb.fit(X, y)\n",
        "plot_learning_curve(fasttext_model_xgb, X, y, cv=3, random_state=2019, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKHtIZKDOJxy"
      },
      "source": [
        "csv = cross_val_score(fasttext_model_xgb, X, y, scoring='accuracy', cv=3).mean()\n",
        "print(csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKNzijYDiB1h"
      },
      "source": [
        "log_to_neptune(fasttext_model_xgb, params, ['text_hero'], ['fasttext', 'text_hero', 'xgb'], cvs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OtN1BboNiSl"
      },
      "source": [
        "#text_hero+numerical, xgb\n",
        "fasttext_model_xgb2 = xgb.XGBClassifier(max_depth=8, n_estimators=100, verbose=0, random_state=0, custom_metric='AUC')\n",
        "fasttext_model_xgb2.fit(X_all, y)\n",
        "plot_learning_curve(fasttext_model_xgb2, X_all, y, cv=3, random_state=2019, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTx2mYr8N8rQ"
      },
      "source": [
        "csv = cross_val_score(fasttext_model_xgb2, X_all, y, scoring='accuracy', cv=3).mean()\n",
        "print(csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXWifZKdiVHp"
      },
      "source": [
        "log_to_neptune(fasttext_model_xgb2, fasttext_model_xgb2, feats, ['fasttext', 'text_hero', 'feature_engineering', 'xgb'], csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRqg1nbaewIs"
      },
      "source": [
        "# **Other**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jztrmYCbOoy_"
      },
      "source": [
        "#prepare kaggle submission\n",
        "test2['simple_tokens'] = test2['text_hero'].map(simple_preprocess)\n",
        "X_test_encoded = get_doc2vec_X(model_w2v, test2['simple_tokens'])\n",
        "test2_w2v2_encoding = pd.DataFrame(X_test_encoded)\n",
        "X_test_all2 = pd.merge(test2_w2v2_encoding,test2[feats],left_index=True, right_index=True)\n",
        "test2['target'] = w2v_model_xgb.predict(X_test_all2).astype(int)\n",
        "test2[ ['id', 'target'] ].to_csv('/content/drive/MyDrive/Konkursy Kaggle/Disaster Tweets/xgb_w2v_allfeats.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nE9Z-dfqIMnM"
      },
      "source": [
        "# Create a Pickle file using serialization \n",
        "pickle_out = open(\"/content/drive/MyDrive/Konkursy Kaggle/Disaster Tweets/model_w2v_4.pkl\",\"wb\")\n",
        "pickle.dump(model_w2v, pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}